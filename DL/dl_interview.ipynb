{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning вопросы\n",
    "\n",
    "<img src='img/tri.jpg' width=500>\n",
    "\n",
    "Оглавление\n",
    "\n",
    "- [Что такое глубокое обучение, и в чем его отличие от традиционного машинного обучения?](#1)\n",
    "- [Как работает нейрон в искусственной нейронной сети (принцип работы)?](#2)\n",
    "- [Функции активации в нейронных сетях](#3)\n",
    "- [Переобучение (Overfitting) и Недообучение (Underfitting)](#4)\n",
    "- [Градиентный спуск](#5)\n",
    "- [Какова роль функции потерь в обучении нейронной сети? Примеры функций потерь для задач NLP.](#1)\n",
    "- [Что такое регуляризация? Какие методы регуляризации наиболее популярны (Dropout, L2)?](#1)\n",
    "- [Архитектуры нейронных сетей](#1)\n",
    "- [Какие типы нейронных сетей вы знаете? Чем они отличаются (RNN, CNN, Transformers)?](#1)\n",
    "- [Что такое рекуррентные нейронные сети (RNN) и для каких задач они подходят?](#1)\n",
    "- [Объясните, как работают LSTM и GRU, и в чем их преимущество перед стандартными RNN.](#1)\n",
    "- [Что такое механизм внимания (Attention)? Почему он важен для NLP?](#1)\n",
    "- [Что такое трансформеры и чем они лучше традиционных RNN/LSTM?](#1)\n",
    "- [Расскажите про архитектуру BERT и его основные особенности.](#1)\n",
    "- [Чем различаются модели GPT и BERT в задачах NLP?](#1)\n",
    "- [Что такое Seq2Seq модели и для каких задач они используются?](#1)\n",
    "- [Работа с текстом в NLP](#1)\n",
    "- [Что такое токенизация и почему она важна для работы моделей NLP?](#1)\n",
    "- [Объясните разницу между Bag of Words (BoW) и word embeddings.](#1)\n",
    "- [Что такое Word2Vec, GloVe, FastText? Как они создают эмбеддинги?](#1)\n",
    "- [Что такое subword-методы (например, Byte Pair Encoding, SentencePiece)?](#1)\n",
    "- [Как трансформеры обрабатывают длинные последовательности текста?](#1)\n",
    "- [Что такое языковое моделирование (Language Modeling)? Объясните маскированное языковое моделирование (Masked LM).](#1)\n",
    "- [Обучение и оптимизация](#1)\n",
    "- [Как справляться с исчезающими и взрывающимися градиентами в нейронных сетях?](#1)\n",
    "- [Что такое transfer learning и как его применять в NLP?](#1)\n",
    "- [Как бороться с дисбалансом классов в задаче классификации текста?](#1)\n",
    "- [Что такое ранняя остановка (Early Stopping) и почему она полезна?](#1)\n",
    "- [Какие метрики используются для оценки качества модели в задачах NLP (например, классификация, машинный перевод)?](#1)\n",
    "- [Чем отличаются train, validation и test выборки? Почему важно их разделение?](#1)\n",
    "- [Оценка и интерпретация](#1)\n",
    "- [Что такое BLEU и ROUGE? Когда они применяются?](#1)\n",
    "- [Как интерпретировать эмбеддинги слов и оценивать их качество?](#1)\n",
    "- [Как можно объяснять предсказания глубоких моделей в задачах NLP?](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "1. <a id=1>Глубокое обучение (Deep Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Глубокое обучение` — это подмножество машинного обучения, основанное на использовании многослойных искусственных нейронных сетей для решения сложных задач, таких как обработка изображений, текста, речи и других типов данных. Оно автоматизирует процесс извлечения признаков (feature extraction) из данных.\n",
    "\n",
    "|Аспект|\tМашинное обучение\t|Глубокое обучение|\n",
    "|-|-|-|\n",
    "|Извлечение признаков\t|Ручное извлечение признаков (Feature Engineering).|\tАвтоматическое извлечение признаков.|\n",
    "|Объем данных\t|Хорошо работает на небольших наборах данных.\t|Требует больших объемов данных для хорошей работы.|\n",
    "|Сложность модели\t|Простые модели (например, линейная регрессия, SVM).\t|Глубокие нейронные сети с большим количеством параметров.|\n",
    "|Зависимость от доменной экспертизы\t|Требует экспертов для определения значимых признаков.\t|Уменьшает зависимость от экспертизы.|\n",
    "|Вычислительные ресурсы\t|Относительно низкие требования к вычислительным мощностям.|\tВысокая потребность в GPU/TPU и больших ресурсах.|\n",
    "|Примеры алгоритмов\t|Линейная регрессия, SVM, Random Forest, KNN.\t|CNN, RNN, Transformers, BERT, GPT.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "2. <a id=2>Как работает нейрон в искусственной нейронной сети </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Каждый нейрон получает несколько входов\n",
    "- Каждое входное значение умножается на соответствующий вес, который определяет важность данного входа.\n",
    "- Вес изменяется в процессе обучения нейронной сети, чтобы минимизировать ошибку\n",
    "\n",
    "$\\boxed{z = \\sum{w \\cdot x_i + b }}$\n",
    "- Взвешенная сумма входов $z$ передается через активационную функцию, которая вводит нелинейность в модель, что позволяет нейронной сети решать сложные задачи.\n",
    "\n",
    "    - $\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "    - $\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "    - $\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Выходы нейронов текущего слоя становятся входами для следующего слоя.\n",
    "- В процессе обучения веса обновляются с помощью метода обратного распространения ошибки (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "3. <a id=3>Функции активации в нейронных сетях </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Функции активации` используются для введения нелинейности в нейронные сети, они преобразуют входной сигнал (взвешенную сумму входов) в выходное значение, которое передается дальше по сети. Выбор функции активации влияет на производительность модели, ее скорость обучения и способность решать разные задачи.\n",
    "\n",
    "$\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "- Преимущества\n",
    "    - Простота и эффективность в вычислениях.\n",
    "    - Быстрая сходимость при обучении глубоких сетей.\n",
    "    - Сохраняет линейность для положительных значений.\n",
    "- Недостатки\n",
    "    - Если вес нейрона приводит к z<0, выход всегда будет 0, и этот нейрон может перестать обновляться.\n",
    "\n",
    "$\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "- Преимущества\n",
    "    - Хорошо подходит для задач бинарной классификации (на выходе вероятность принадлежности к классу)\n",
    "- Недостатки\n",
    "    - При больших или малых значениях z, производная становится близкой к 0, что замедляет обучение.\n",
    "    - Среднее значение выхода не равно 0, что усложняет обучение\n",
    "\n",
    "$\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Преимущества\n",
    "    - Более широкий диапазон значений, чем у Sigmoid [-1, 1]\n",
    "    - Среднее значение выхода равно 0, что улучшает обучение\n",
    "- Недостатки\n",
    "    - Страдает от проблемы исчезающего градиента при больших значениях 𝑧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "4. <a id=4>Переобучение (Overfitting) и Недообучение (Underfitting) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Переобучение` возникает, когда модель слишком хорошо адаптируется к тренировочным данным и теряет способность обобщать информацию на новых, ранее не виденных данных. Модель \"запоминает\" данные, включая шум и случайные отклонения, вместо того чтобы находить общие закономерности.\n",
    "\n",
    "`Признаки переобучения`:\n",
    "- Высокая точность на тренировочной выборке и низкая на тестовой\n",
    "\n",
    "`Причины переобучения`:\n",
    "- Слишком сложная модель\n",
    "- Недостаточный объем данных.\n",
    "- Отсутствие регуляризации\n",
    "- Слишком долгое обучение модели.\n",
    "\n",
    "`Методы борьбы с переобучением:`\n",
    "- Увеличение данных\n",
    "- Аугментация данных\n",
    "- Добавление штрафа за слишком большие веса в функции потерь (регуляризация)\n",
    "- Использование dropout\n",
    "- Уменьшение количества слоев или нейронов\n",
    "- Использование меньшего числа параметров\n",
    "- Прекращение обучения, когда ошибка на валидационной выборке начинает расти\n",
    "- Кросс-валидация\n",
    "`Недообучение` возникает, когда модель не может выявить зависимости даже в тренировочных данных, из-за чего её предсказания остаются неточными как на тренировочной, так и на тестовой выборках.\n",
    "\n",
    "`Признаки недообучения`:\n",
    "- Низкая точность как на тренировочной, так и на тестовой выборке.\n",
    "- Ошибка практически не уменьшается при увеличении количества итераций обучения.\n",
    "Причины недообучения:\n",
    "- Слишком простая модель\n",
    "- Недостаточное количество эпох обучения\n",
    "- Неподходящая архитектура или функции активации\n",
    "- Плохая подготовка данных\n",
    "\n",
    "`Методы борьбы с недообучением:`\n",
    "- Добавление новых слоев или нейронов\n",
    "- Использование более сложной архитектуры\n",
    "- Увеличение количества эпох\n",
    "- Настройка параметров обучения, таких как скорость обучения (learning rate), оптимизатор, размер батча\n",
    "- Улучшение качества данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "5. <a id=5>Градиентный спуск </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Градиентный спуск` — это оптимизационный алгоритм, используемый для минимизации функции потерь в процессе обучения нейронной сети. Его цель — найти такие параметры модели (веса и смещения), которые обеспечивают минимальную ошибку предсказаний модели на тренировочных данных.\n",
    "\n",
    "- Функция потерь (Loss Function): Определяет, насколько сильно предсказания модели отличаются от реальных значений.\n",
    "- Градиент: Показывает направление наибольшего увеличения функции потерь. Для минимизации мы движемся в противоположном направлении.\n",
    "- Шаг обновления параметров: $\\boxed{w - \\lambda \\cdot \\frac{∂L}{∂w}}$\n",
    "\n",
    "Виды градиентного спуска:\n",
    "- Полный градиентный спуск (`Batch Gradient Descent`): Использует все тренировочные данные для вычисления градиента на каждом шаге.\n",
    "    - Точный расчет градиента\n",
    "    - Долгое время обучения на больших данных.\n",
    "    - Высокие затраты памяти\n",
    "\n",
    "- Стохастический градиентный спуск (`SGD`): Обновляет параметры на основе одного случайного примера из тренировочных данных на каждом шаге.\n",
    "    - Быстрее вычисляется на больших данных\n",
    "    - Cлучайность помогает избежать локальных минимумов.\n",
    "    - Может колебаться вокруг оптимального решения\n",
    "    - Высокая дисперсия в направлении обновлений\n",
    "\n",
    "- `Мини-батч` градиентный спуск: Делит данные на небольшие группы (батчи) и вычисляет градиент на основе одного батча\n",
    "    - Компромисс между точностью и скорост\n",
    "    - Эффективно работает на больших данных\n",
    "    - Требует настройки размера батча.\n",
    "\n",
    "- `Momentum`: обавляет инерцию к обновлению параметров, чтобы ускорить обучение\n",
    "    - Помогает пересекать плато и ускоряет сходимость\n",
    "- `RMSProp` (Root Mean Square Propagation): Использует скользящее среднее квадратов градиентов для адаптации скорости обучения.\n",
    "    - Хорошо работает для задач с разреженными признаками.\n",
    "\n",
    "- `Adam` (Adaptive Moment Estimation): Комбинирует идеи RMSProp и Momentum\n",
    "    - "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
