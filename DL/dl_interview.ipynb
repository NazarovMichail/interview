{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning вопросы\n",
    "\n",
    "<img src='img/tri.jpg' width=500>\n",
    "\n",
    "Оглавление\n",
    "\n",
    "- [Что такое глубокое обучение, и в чем его отличие от традиционного машинного обучения?](#1)\n",
    "- [Как работает нейрон в искусственной нейронной сети (принцип работы)?](#2)\n",
    "- [Функции активации в нейронных сетях](#3)\n",
    "- [Переобучение (Overfitting) и Недообучение (Underfitting)](#4)\n",
    "- [Градиентный спуск](#5)\n",
    "- [Какова роль функции потерь в обучении нейронной сети? Примеры функций потерь для задач NLP.](#6)\n",
    "- [Регуляризация](#7)\n",
    "- [Архитектуры нейронных сетей](#8)\n",
    "- [Какие типы нейронных сетей вы знаете? Чем они отличаются (RNN, CNN, Transformers)?](#1)\n",
    "- [Что такое рекуррентные нейронные сети (RNN) и для каких задач они подходят?](#1)\n",
    "- [Объясните, как работают LSTM и GRU, и в чем их преимущество перед стандартными RNN.](#1)\n",
    "- [Что такое механизм внимания (Attention)? Почему он важен для NLP?](#1)\n",
    "- [Что такое трансформеры и чем они лучше традиционных RNN/LSTM?](#1)\n",
    "- [Расскажите про архитектуру BERT и его основные особенности.](#1)\n",
    "- [Чем различаются модели GPT и BERT в задачах NLP?](#1)\n",
    "- [Что такое Seq2Seq модели и для каких задач они используются?](#1)\n",
    "- [Работа с текстом в NLP](#1)\n",
    "- [Что такое токенизация и почему она важна для работы моделей NLP?](#1)\n",
    "- [Объясните разницу между Bag of Words (BoW) и word embeddings.](#1)\n",
    "- [Что такое Word2Vec, GloVe, FastText? Как они создают эмбеддинги?](#1)\n",
    "- [Что такое subword-методы (например, Byte Pair Encoding, SentencePiece)?](#1)\n",
    "- [Как трансформеры обрабатывают длинные последовательности текста?](#1)\n",
    "- [Что такое языковое моделирование (Language Modeling)? Объясните маскированное языковое моделирование (Masked LM).](#1)\n",
    "- [Обучение и оптимизация](#1)\n",
    "- [Как справляться с исчезающими и взрывающимися градиентами в нейронных сетях?](#1)\n",
    "- [Что такое transfer learning и как его применять в NLP?](#1)\n",
    "- [Как бороться с дисбалансом классов в задаче классификации текста?](#1)\n",
    "- [Что такое ранняя остановка (Early Stopping) и почему она полезна?](#1)\n",
    "- [Какие метрики используются для оценки качества модели в задачах NLP (например, классификация, машинный перевод)?](#1)\n",
    "- [Чем отличаются train, validation и test выборки? Почему важно их разделение?](#1)\n",
    "- [Оценка и интерпретация](#1)\n",
    "- [Что такое BLEU и ROUGE? Когда они применяются?](#1)\n",
    "- [Как интерпретировать эмбеддинги слов и оценивать их качество?](#1)\n",
    "- [Как можно объяснять предсказания глубоких моделей в задачах NLP?](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "1. <a id=1>Глубокое обучение (Deep Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Глубокое обучение` — это подмножество машинного обучения, основанное на использовании многослойных искусственных нейронных сетей для решения сложных задач, таких как обработка изображений, текста, речи и других типов данных. Оно автоматизирует процесс извлечения признаков (feature extraction) из данных.\n",
    "\n",
    "|Аспект|\tМашинное обучение\t|Глубокое обучение|\n",
    "|-|-|-|\n",
    "|Извлечение признаков\t|Ручное извлечение признаков (Feature Engineering).|\tАвтоматическое извлечение признаков.|\n",
    "|Объем данных\t|Хорошо работает на небольших наборах данных.\t|Требует больших объемов данных для хорошей работы.|\n",
    "|Сложность модели\t|Простые модели (например, линейная регрессия, SVM).\t|Глубокие нейронные сети с большим количеством параметров.|\n",
    "|Зависимость от доменной экспертизы\t|Требует экспертов для определения значимых признаков.\t|Уменьшает зависимость от экспертизы.|\n",
    "|Вычислительные ресурсы\t|Относительно низкие требования к вычислительным мощностям.|\tВысокая потребность в GPU/TPU и больших ресурсах.|\n",
    "|Примеры алгоритмов\t|Линейная регрессия, SVM, Random Forest, KNN.\t|CNN, RNN, Transformers, BERT, GPT.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "2. <a id=2>Как работает нейрон в искусственной нейронной сети </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Каждый нейрон получает несколько входов\n",
    "- Каждое входное значение умножается на соответствующий вес, который определяет важность данного входа.\n",
    "- Вес изменяется в процессе обучения нейронной сети, чтобы минимизировать ошибку\n",
    "\n",
    "$\\boxed{z = \\sum{w \\cdot x_i + b }}$\n",
    "- Взвешенная сумма входов $z$ передается через активационную функцию, которая вводит нелинейность в модель, что позволяет нейронной сети решать сложные задачи.\n",
    "\n",
    "    - $\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "    - $\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "    - $\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Выходы нейронов текущего слоя становятся входами для следующего слоя.\n",
    "- В процессе обучения веса обновляются с помощью метода обратного распространения ошибки (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "3. <a id=3>Функции активации в нейронных сетях </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Функции активации` используются для введения нелинейности в нейронные сети, они преобразуют входной сигнал (взвешенную сумму входов) в выходное значение, которое передается дальше по сети. Выбор функции активации влияет на производительность модели, ее скорость обучения и способность решать разные задачи.\n",
    "\n",
    "$\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "- Преимущества\n",
    "    - Простота и эффективность в вычислениях.\n",
    "    - Быстрая сходимость при обучении глубоких сетей.\n",
    "    - Сохраняет линейность для положительных значений.\n",
    "- Недостатки\n",
    "    - Если вес нейрона приводит к z<0, выход всегда будет 0, и этот нейрон может перестать обновляться.\n",
    "\n",
    "$\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "- Преимущества\n",
    "    - Хорошо подходит для задач бинарной классификации (на выходе вероятность принадлежности к классу)\n",
    "- Недостатки\n",
    "    - При больших или малых значениях z, производная становится близкой к 0, что замедляет обучение.\n",
    "        - $\\boxed{\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))}$, При значениях z 0 и 1 градиент = 0\n",
    "    - Среднее значение выхода не равно 0, что усложняет обучение\n",
    "\n",
    "$\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Преимущества\n",
    "    - Более широкий диапазон значений, чем у Sigmoid [-1, 1]\n",
    "    - Среднее значение выхода равно 0, что улучшает обучение\n",
    "- Недостатки\n",
    "    - Страдает от проблемы исчезающего градиента при больших значениях 𝑧\n",
    "        - $\\boxed{Tanh'(z) = 1 - Tanh^2(z)}$, При значениях z 1 и -1 градиент = 0\n",
    "\n",
    "$\\boxed{GeLU(x) = x \\cdot \\Phi(x)}$\n",
    "- $\\Phi(x)$ : комулятивная функция распределения нормального гауссовского распределения\n",
    "- GeLU похож на ReLU, но с более плавным переходом в области x < 0\n",
    "\n",
    "$\\boxed{SiLU(x) = x \\cdot \\sigma(x)}$\n",
    "- для отрицательных входов сохраняется небольшая градиентная информация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "4. <a id=4>Переобучение (Overfitting) и Недообучение (Underfitting) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Переобучение` возникает, когда модель слишком хорошо адаптируется к тренировочным данным и теряет способность обобщать информацию на новых, ранее не виденных данных. Модель \"запоминает\" данные, включая шум и случайные отклонения, вместо того чтобы находить общие закономерности.\n",
    "\n",
    "`Признаки переобучения`:\n",
    "- Высокая точность на тренировочной выборке и низкая на тестовой\n",
    "\n",
    "`Причины переобучения`:\n",
    "- Слишком сложная модель\n",
    "- Недостаточный объем данных.\n",
    "- Отсутствие регуляризации\n",
    "- Слишком долгое обучение модели.\n",
    "\n",
    "`Методы борьбы с переобучением:`\n",
    "- Увеличение данных\n",
    "- Аугментация данных\n",
    "- Добавление штрафа за слишком большие веса в функции потерь (регуляризация)\n",
    "- Использование dropout\n",
    "- Уменьшение количества слоев или нейронов\n",
    "- Использование меньшего числа параметров\n",
    "- Прекращение обучения, когда ошибка на валидационной выборке начинает расти\n",
    "- Кросс-валидация\n",
    "\n",
    "`Недообучение` возникает, когда модель не может выявить зависимости даже в тренировочных данных, из-за чего её предсказания остаются неточными как на тренировочной, так и на тестовой выборках.\n",
    "\n",
    "`Признаки недообучения`:\n",
    "- Низкая точность как на тренировочной, так и на тестовой выборке.\n",
    "- Ошибка практически не уменьшается при увеличении количества итераций обучения.\n",
    "Причины недообучения:\n",
    "- Слишком простая модель\n",
    "- Недостаточное количество эпох обучения\n",
    "- Неподходящая архитектура или функции активации\n",
    "- Плохая подготовка данных\n",
    "\n",
    "`Методы борьбы с недообучением:`\n",
    "- Добавление новых слоев или нейронов\n",
    "- Использование более сложной архитектуры\n",
    "- Увеличение количества эпох\n",
    "- Настройка параметров обучения, таких как скорость обучения (learning rate), оптимизатор, размер батча\n",
    "- Улучшение качества данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "5. <a id=5>Градиентный спуск </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Градиентный спуск` — это оптимизационный алгоритм, используемый для минимизации функции потерь в процессе обучения нейронной сети. Его цель — найти такие параметры модели (веса и смещения), которые обеспечивают минимальную ошибку предсказаний модели на тренировочных данных.\n",
    "\n",
    "- Функция потерь (Loss Function): Определяет, насколько сильно предсказания модели отличаются от реальных значений.\n",
    "- Градиент: Показывает направление наибольшего увеличения функции потерь. Для минимизации мы движемся в противоположном направлении.\n",
    "- Шаг обновления параметров: $\\boxed{x_1 - \\gamma_i \\cdot grad}$\n",
    "\n",
    "Виды градиентного спуска:\n",
    "- Полный градиентный спуск (`Batch Gradient Descent`): Использует все тренировочные данные для вычисления градиента на каждом шаге.\n",
    "    - Точный расчет градиента\n",
    "    - Долгое время обучения на больших данных.\n",
    "    - Высокие затраты памяти\n",
    "\n",
    "- Стохастический градиентный спуск (`SGD`): Обновляет параметры на основе одного случайного примера из тренировочных данных на каждом шаге.\n",
    "    - Быстрее вычисляется на больших данных\n",
    "    - Cлучайность помогает избежать локальных минимумов.\n",
    "    - Может колебаться вокруг оптимального решения\n",
    "    - Высокая дисперсия в направлении обновлений\n",
    "\n",
    "- `Мини-батч` градиентный спуск: Делит данные на небольшие группы (батчи) и вычисляет градиент на основе одного батча\n",
    "    - Компромисс между точностью и скоростью\n",
    "    - Эффективно работает на больших данных\n",
    "    - Требует настройки размера батча.\n",
    "\n",
    "- `Momentum`: добавляет инерцию к обновлению параметров, чтобы ускорить обучение\n",
    "    - Помогает пересекать плато и ускоряет сходимость\n",
    "\n",
    "    - $\\boxed{x_{i+1} = x_i - \\gamma_i \\cdot grad + \\alpha \\cdot \\triangle x_i}$, где\n",
    "        - $\\triangle x_i = x_i - x_{i-1}$\n",
    "        - $\\alpha \\in [0,1]$ : выбранное некоторое число (`постоянная импульса`) \n",
    "- `RMSProp` (Root Mean Square Propagation): Использует скользящее среднее квадратов градиентов для адаптации скорости обучения.\n",
    "    - Хорошо работает для задач с разреженными признаками. \n",
    "\n",
    "    - $\\boxed{x_{i+1} = x_i - \\frac{\\gamma_i}{\\sqrt{m}} \\cdot grad}$, где\n",
    "        - $m = \\beta \\cdot g + (1 - \\beta) \\cdot g^2$: Скользящее среднее квадратов градиентов\n",
    "- `Adam` (Adaptive Moment Estimation): Комбинирует идеи RMSProp и Momentum\n",
    "    - Быстрая сходимость.\n",
    "    - Может приводить к переобучению, если плохо настроены гиперпараметры\n",
    "    - $\\boxed{x_{i+1} = x_i - \\frac{\\gamma_i}{\\sqrt{m}} \\cdot v}$, где\n",
    "        - $m = \\beta_1 \\cdot g + (1 - \\beta_1) \\cdot g^2$: Скользящее среднее квадратов градиентов\n",
    "        - $v = \\beta_2 \\cdot g + (1 - \\beta_2) \\cdot g$: Скользящее среднее квадратов градиентов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "6. <a id=6>Функция потерь в обучении нейронной сети </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Функция потерь` (Loss Function) - функция, которая дает численную оценку качества предсказаний модели, чтобы алгоритм оптимизации (например, градиентный спуск) мог минимизировать эту ошибку.\n",
    "- После каждого прохода данных через сеть функция потерь вычисляет ошибку между реальными и предсказанными значениями.\n",
    "- Градиент функции потерь используется для обновления параметров модели с целью минимизации этой ошибки.\n",
    "\n",
    "Функции потерь\n",
    "\n",
    "- `Кросс-энтропия` (Cross-Entropy Loss): Используется, когда цель — предсказать вероятность для одного из нескольких классов.\n",
    "\n",
    "    - $\\boxed{L = -\\frac{1}{N}\\sum^N \\sum^K y_{i, k} \\cdot log(\\hat{y}_{i, k})}$\n",
    "    - Классификация тональности текста \n",
    "    - Категоризация документов\n",
    "- `Перекрестная энтропия с маскированием`: Используется для задач, где длина последовательностей может различаться (например, Seq2Seq модели)\n",
    "     - $\\boxed{L = -\\frac{1}{N}\\sum^N \\sum^T m_{i,t} \\cdot y_{i, t} \\cdot log(\\hat{y}_{i, t})}$\n",
    "        - $m_{i,t}$: маска (равна 1 для реальных токенов, 0 для паддинга)\n",
    "    - Перевод текста\n",
    "    - Генерация текста (например, автозаполнение).\n",
    "\n",
    "- `Негативное логарифмическое правдоподобие` (Negative Log-Likelihood, NLL): Используется в языковых моделях, чтобы минимизировать вероятность ошибок в предсказании следующего слова.\n",
    "    - $\\boxed{L = -\\frac{1}{N}\\sum^N log(y_i | x_i)}$\n",
    "    - Предсказание следующего слова в предложении.\n",
    "- `Контрастивная потеря` (Contrastive Loss): Применяется для обучения моделей, которые работают с эмбеддингами текста\n",
    "    - $\\boxed{L = (1 - y) \\cdot max(0, d-m) + y\\cdot max(0, m-d)}$\n",
    "        - $d$: расстояние между эмбеддингами\n",
    "        - $m$: пороговое значение\n",
    "        - $y$: истинное значение (0 или 1)\n",
    "    - Поиск похожих текстов\n",
    "\n",
    "- `Среднеквадратичная ошибка` (Mean Squared Error, MSE): Используется, когда требуется предсказать числовое значение\n",
    "    - $\\boxed{L = \\frac{1}{N}\\sum^N (y_i - \\hat{y}_{i})^2}$\n",
    "    - Оценка длины текста.\n",
    "    - Прогнозирование следующего слова в тексте с указанием его частоты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "7. <a id=7>Регуляризация</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Регуляризация` — это совокупность методов, которые предотвращают переобучение (overfitting) модели и помогают ей обобщать данные.   \n",
    "Цель регуляризации — сделать модель более устойчивой к шуму и менее зависимой от конкретных данных в тренировочной выборке, чтобы она хорошо работала на новых данных.  \n",
    "Регуляризация добавляет `ограничения` на `веса` модели или `структуру` нейронной сети, чтобы минимизировать вероятность переобучения.\n",
    "\n",
    "- `L2-регуляризация` (Ridge Regularization)\n",
    "    - $\\boxed{L = L + \\lambda \\cdot \\sum w_i^2}$\n",
    "    - снижает вероятность переобучения, не обнуляя полностью веса\n",
    "    - Чем больше λ, тем сильнее штраф и меньше веса\n",
    "\n",
    "- `L1-регуляризация` (Lasso  Regularization)\n",
    "    - $\\boxed{L = L + \\lambda \\cdot \\sum | w_i|}$\n",
    "    - Полезна для отбора признаков (feature selection), так как обнуляет незначимые веса\n",
    "    - Используется реже в нейронных сетях по сравнению с L2\n",
    "- `Dropout`: На этапе обучения случайно отключает нейроны (и их связи) в сети с заданной вероятностью p, что препятствует избыточной `зависимости` нейронов друг от друга\n",
    "    - Снижает вероятность переобучения за счет создания разнообразных подмножеств сети\n",
    "    - На этапе тестирования все нейроны используются, но их веса уменьшаются пропорционально вероятности p\n",
    "- `Batch Normalization`: Нормализует активации нейронов внутри слоя, стабилизируя обучение, что помогает сети избегать сильных отклонений `значений активаций`\n",
    "\n",
    "- `Data Augmentation`: Искусственное увеличение тренировочных данных за счет их модификаций \n",
    "    - Синонимизация слов.\n",
    "    - Удаление/перестановка слов в предложениях\n",
    "    - Перевод на другой язык\n",
    "\n",
    "- `Ранняя остановка (Early Stopping)`: Прекращает обучение, если ошибка на `валидационной` выборке перестает уменьшаться, даже если на тренировочной выборке она продолжает снижаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "8. <a id=8>Архитектуры нейронных сетей</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Архитектура**       | **Применение**                     | **Плюсы**                                                                                   | **Минусы**                                                                                  |\n",
    "|-----------------------|------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| **FCNN**             | Табличные данные, простые задачи. | - Простота реализации. <br> - Хорошо работает на структурированных данных.                 | - Неподходяща для неструктурированных данных (изображения, текст). <br> - Много параметров. |\n",
    "| **CNN**              | Обработка изображений.            | - Автоматическое извлечение пространственных признаков. <br> - Эффективна для изображений.  | - Ограничена в работе с последовательными данными (например, текст).                       |\n",
    "| **RNN**              | Последовательности.               | - Учитывает временные зависимости. <br> - Подходит для работы с последовательными данными. | - Проблемы исчезающих и взрывающихся градиентов. <br> - Плохо работает с длинными последовательностями. |\n",
    "| **LSTM/GRU**         | Последовательности.               | - Решает проблему длинных зависимостей. <br> - Подходит для текста, временных рядов.        | - Более сложные вычисления. <br> - Медленная обработка по сравнению с трансформерами.      |\n",
    "| **Transformers**     | Текст, последовательности.        | - Эффективны для длинных последовательностей. <br> - Высокая параллелизация.               | - Высокие вычислительные затраты. <br> - Требуют большого объема данных для обучения.      |\n",
    "| **Autoencoders**     | Сжатие данных.                    | - Компактное представление данных. <br> - Применимы для обнаружения аномалий.              | - Не всегда сохраняют детали данных. <br> - Зависимость от правильной настройки.           |\n",
    "| **GANs**             | Генерация данных.                 | - Генерация реалистичных данных. <br> - Применимы для увеличения данных.                   | - Сложность обучения (нестабильная сходимость). <br> - Чувствительность к настройке.       |\n",
    "| **GNN**              | Графовые структуры.               | - Учет сложных связей между объектами. <br> - Подходит для анализа сетей и рекомендаций.    | - Высокая вычислительная сложность на больших графах. <br> - Требует специализированных данных. |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
