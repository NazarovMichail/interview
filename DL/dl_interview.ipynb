{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning вопросы\n",
    "\n",
    "<img src='img/tri.jpg' width=500>\n",
    "\n",
    "Оглавление\n",
    "\n",
    "- [Что такое глубокое обучение, и в чем его отличие от традиционного машинного обучения?](#1)\n",
    "- [Как работает нейрон в искусственной нейронной сети (принцип работы)?](#2)\n",
    "- [Функции активации в нейронных сетях](#3)\n",
    "- [Переобучение (Overfitting) и Недообучение (Underfitting)](#4)\n",
    "- [Градиентный спуск](#5)\n",
    "- [Какова роль функции потерь в обучении нейронной сети? Примеры функций потерь для задач NLP.](#6)\n",
    "- [Регуляризация](#7)\n",
    "- [Архитектуры нейронных сетей](#8)\n",
    "- [Рекуррентные нейронные сети (RNN)](#9)\n",
    "- [Объясните, как работают LSTM и GRU, и в чем их преимущество перед стандартными RNN.](#10)\n",
    "- [Механизм внимания (Attention)](#11)\n",
    "- [Что такое трансформеры и чем они лучше традиционных RNN/LSTM?](#1)\n",
    "- [Расскажите про архитектуру BERT и его основные особенности.](#1)\n",
    "- [Чем различаются модели GPT и BERT в задачах NLP?](#1)\n",
    "- [Что такое Seq2Seq модели и для каких задач они используются?](#1)\n",
    "- [Работа с текстом в NLP](#1)\n",
    "- [Что такое токенизация и почему она важна для работы моделей NLP?](#1)\n",
    "- [Объясните разницу между Bag of Words (BoW) и word embeddings.](#1)\n",
    "- [Что такое Word2Vec, GloVe, FastText? Как они создают эмбеддинги?](#1)\n",
    "- [Что такое subword-методы (например, Byte Pair Encoding, SentencePiece)?](#1)\n",
    "- [Как трансформеры обрабатывают длинные последовательности текста?](#1)\n",
    "- [Что такое языковое моделирование (Language Modeling)? Объясните маскированное языковое моделирование (Masked LM).](#1)\n",
    "- [Обучение и оптимизация](#1)\n",
    "- [Как справляться с исчезающими и взрывающимися градиентами в нейронных сетях?](#1)\n",
    "- [Что такое transfer learning и как его применять в NLP?](#1)\n",
    "- [Как бороться с дисбалансом классов в задаче классификации текста?](#1)\n",
    "- [Что такое ранняя остановка (Early Stopping) и почему она полезна?](#1)\n",
    "- [Какие метрики используются для оценки качества модели в задачах NLP (например, классификация, машинный перевод)?](#1)\n",
    "- [Чем отличаются train, validation и test выборки? Почему важно их разделение?](#1)\n",
    "- [Оценка и интерпретация](#1)\n",
    "- [Что такое BLEU и ROUGE? Когда они применяются?](#1)\n",
    "- [Как интерпретировать эмбеддинги слов и оценивать их качество?](#1)\n",
    "- [Как можно объяснять предсказания глубоких моделей в задачах NLP?](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "1. <a id=1>Глубокое обучение (Deep Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Глубокое обучение` — это подмножество машинного обучения, основанное на использовании многослойных искусственных нейронных сетей для решения сложных задач, таких как обработка изображений, текста, речи и других типов данных. Оно автоматизирует процесс извлечения признаков (feature extraction) из данных.\n",
    "\n",
    "|Аспект|\tМашинное обучение\t|Глубокое обучение|\n",
    "|-|-|-|\n",
    "|Извлечение признаков\t|Ручное извлечение признаков (Feature Engineering).|\tАвтоматическое извлечение признаков.|\n",
    "|Объем данных\t|Хорошо работает на небольших наборах данных.\t|Требует больших объемов данных для хорошей работы.|\n",
    "|Сложность модели\t|Простые модели (например, линейная регрессия, SVM).\t|Глубокие нейронные сети с большим количеством параметров.|\n",
    "|Зависимость от доменной экспертизы\t|Требует экспертов для определения значимых признаков.\t|Уменьшает зависимость от экспертизы.|\n",
    "|Вычислительные ресурсы\t|Относительно низкие требования к вычислительным мощностям.|\tВысокая потребность в GPU/TPU и больших ресурсах.|\n",
    "|Примеры алгоритмов\t|Линейная регрессия, SVM, Random Forest, KNN.\t|CNN, RNN, Transformers, BERT, GPT.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "2. <a id=2>Как работает нейрон в искусственной нейронной сети </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Каждый нейрон получает несколько входов\n",
    "- Каждое входное значение умножается на соответствующий вес, который определяет важность данного входа.\n",
    "- Вес изменяется в процессе обучения нейронной сети, чтобы минимизировать ошибку\n",
    "\n",
    "$\\boxed{z = \\sum{w \\cdot x_i + b }}$\n",
    "- Взвешенная сумма входов $z$ передается через активационную функцию, которая вводит нелинейность в модель, что позволяет нейронной сети решать сложные задачи.\n",
    "\n",
    "    - $\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "    - $\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "    - $\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Выходы нейронов текущего слоя становятся входами для следующего слоя.\n",
    "- В процессе обучения веса обновляются с помощью метода обратного распространения ошибки (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "3. <a id=3>Функции активации в нейронных сетях </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Функции активации` используются для введения нелинейности в нейронные сети, они преобразуют входной сигнал (взвешенную сумму входов) в выходное значение, которое передается дальше по сети. Выбор функции активации влияет на производительность модели, ее скорость обучения и способность решать разные задачи.\n",
    "\n",
    "$\\boxed{ReLU = max(0, z)}$\n",
    "\n",
    "- Преимущества\n",
    "    - Простота и эффективность в вычислениях.\n",
    "    - Быстрая сходимость при обучении глубоких сетей.\n",
    "    - Сохраняет линейность для положительных значений.\n",
    "- Недостатки\n",
    "    - Если вес нейрона приводит к z<0, выход всегда будет 0, и этот нейрон может перестать обновляться.\n",
    "\n",
    "$\\boxed{Sigmoid = \\frac{1}{1 + e^{-z}}}$\n",
    "- Преимущества\n",
    "    - Хорошо подходит для задач бинарной классификации (на выходе вероятность принадлежности к классу)\n",
    "- Недостатки\n",
    "    - При больших или малых значениях z, производная становится близкой к 0, что замедляет обучение.\n",
    "        - $\\boxed{\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))}$, При значениях z 0 и 1 градиент = 0\n",
    "    - Среднее значение выхода не равно 0, что усложняет обучение\n",
    "\n",
    "$\\boxed{Tanh  = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}}$\n",
    "\n",
    "- Преимущества\n",
    "    - Более широкий диапазон значений, чем у Sigmoid [-1, 1]\n",
    "    - Среднее значение выхода равно 0, что улучшает обучение\n",
    "- Недостатки\n",
    "    - Страдает от проблемы исчезающего градиента при больших значениях 𝑧\n",
    "        - $\\boxed{Tanh'(z) = 1 - Tanh^2(z)}$, При значениях z 1 и -1 градиент = 0\n",
    "\n",
    "$\\boxed{GeLU(x) = x \\cdot \\Phi(x)}$\n",
    "- $\\Phi(x)$ : комулятивная функция распределения нормального гауссовского распределения\n",
    "- GeLU похож на ReLU, но с более плавным переходом в области x < 0\n",
    "\n",
    "$\\boxed{SiLU(x) = x \\cdot \\sigma(x)}$\n",
    "- для отрицательных входов сохраняется небольшая градиентная информация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "4. <a id=4>Переобучение (Overfitting) и Недообучение (Underfitting) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Переобучение` возникает, когда модель слишком хорошо адаптируется к тренировочным данным и теряет способность обобщать информацию на новых, ранее не виденных данных. Модель \"запоминает\" данные, включая шум и случайные отклонения, вместо того чтобы находить общие закономерности.\n",
    "\n",
    "`Признаки переобучения`:\n",
    "- Высокая точность на тренировочной выборке и низкая на тестовой\n",
    "\n",
    "`Причины переобучения`:\n",
    "- Слишком сложная модель\n",
    "- Недостаточный объем данных.\n",
    "- Отсутствие регуляризации\n",
    "- Слишком долгое обучение модели.\n",
    "\n",
    "`Методы борьбы с переобучением:`\n",
    "- Увеличение данных\n",
    "- Аугментация данных\n",
    "- Добавление штрафа за слишком большие веса в функции потерь (регуляризация)\n",
    "- Использование dropout\n",
    "- Уменьшение количества слоев или нейронов\n",
    "- Использование меньшего числа параметров\n",
    "- Прекращение обучения, когда ошибка на валидационной выборке начинает расти\n",
    "- Кросс-валидация\n",
    "\n",
    "`Недообучение` возникает, когда модель не может выявить зависимости даже в тренировочных данных, из-за чего её предсказания остаются неточными как на тренировочной, так и на тестовой выборках.\n",
    "\n",
    "`Признаки недообучения`:\n",
    "- Низкая точность как на тренировочной, так и на тестовой выборке.\n",
    "- Ошибка практически не уменьшается при увеличении количества итераций обучения.\n",
    "Причины недообучения:\n",
    "- Слишком простая модель\n",
    "- Недостаточное количество эпох обучения\n",
    "- Неподходящая архитектура или функции активации\n",
    "- Плохая подготовка данных\n",
    "\n",
    "`Методы борьбы с недообучением:`\n",
    "- Добавление новых слоев или нейронов\n",
    "- Использование более сложной архитектуры\n",
    "- Увеличение количества эпох\n",
    "- Настройка параметров обучения, таких как скорость обучения (learning rate), оптимизатор, размер батча\n",
    "- Улучшение качества данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "5. <a id=5>Градиентный спуск </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Градиентный спуск` — это оптимизационный алгоритм, используемый для минимизации функции потерь в процессе обучения нейронной сети. Его цель — найти такие параметры модели (веса и смещения), которые обеспечивают минимальную ошибку предсказаний модели на тренировочных данных.\n",
    "\n",
    "- Функция потерь (Loss Function): Определяет, насколько сильно предсказания модели отличаются от реальных значений.\n",
    "- Градиент: Показывает направление наибольшего увеличения функции потерь. Для минимизации мы движемся в противоположном направлении.\n",
    "- Шаг обновления параметров: $\\boxed{x_1 - \\gamma_i \\cdot grad}$\n",
    "\n",
    "Виды градиентного спуска:\n",
    "- Полный градиентный спуск (`Batch Gradient Descent`): Использует все тренировочные данные для вычисления градиента на каждом шаге.\n",
    "    - Точный расчет градиента\n",
    "    - Долгое время обучения на больших данных.\n",
    "    - Высокие затраты памяти\n",
    "\n",
    "- Стохастический градиентный спуск (`SGD`): Обновляет параметры на основе одного случайного примера из тренировочных данных на каждом шаге.\n",
    "    - Быстрее вычисляется на больших данных\n",
    "    - Cлучайность помогает избежать локальных минимумов.\n",
    "    - Может колебаться вокруг оптимального решения\n",
    "    - Высокая дисперсия в направлении обновлений\n",
    "\n",
    "- `Мини-батч` градиентный спуск: Делит данные на небольшие группы (батчи) и вычисляет градиент на основе одного батча\n",
    "    - Компромисс между точностью и скоростью\n",
    "    - Эффективно работает на больших данных\n",
    "    - Требует настройки размера батча.\n",
    "\n",
    "- `Momentum`: добавляет инерцию к обновлению параметров, чтобы ускорить обучение\n",
    "    - Помогает пересекать плато и ускоряет сходимость\n",
    "\n",
    "    - $\\boxed{x_{i+1} = x_i - \\gamma_i \\cdot grad + \\alpha \\cdot \\triangle x_i}$, где\n",
    "        - $\\triangle x_i = x_i - x_{i-1}$\n",
    "        - $\\alpha \\in [0,1]$ : выбранное некоторое число (`постоянная импульса`) \n",
    "- `RMSProp` (Root Mean Square Propagation): Использует скользящее среднее квадратов градиентов для адаптации скорости обучения.\n",
    "    - Хорошо работает для задач с разреженными признаками. \n",
    "\n",
    "    - $\\boxed{x_{i+1} = x_i - \\frac{\\gamma_i}{\\sqrt{m}} \\cdot grad}$, где\n",
    "        - $m = \\beta \\cdot g + (1 - \\beta) \\cdot g^2$: Скользящее среднее квадратов градиентов\n",
    "- `Adam` (Adaptive Moment Estimation): Комбинирует идеи RMSProp и Momentum\n",
    "    - Быстрая сходимость.\n",
    "    - Может приводить к переобучению, если плохо настроены гиперпараметры\n",
    "    - $\\boxed{x_{i+1} = x_i - \\frac{\\gamma_i}{\\sqrt{m}} \\cdot v}$, где\n",
    "        - $m = \\beta_1 \\cdot g + (1 - \\beta_1) \\cdot g^2$: Скользящее среднее квадратов градиентов\n",
    "        - $v = \\beta_2 \\cdot g + (1 - \\beta_2) \\cdot g$: Скользящее среднее квадратов градиентов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "6. <a id=6>Функция потерь в обучении нейронной сети </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Функция потерь` (Loss Function) - функция, которая дает численную оценку качества предсказаний модели, чтобы алгоритм оптимизации (например, градиентный спуск) мог минимизировать эту ошибку.\n",
    "- После каждого прохода данных через сеть функция потерь вычисляет ошибку между реальными и предсказанными значениями.\n",
    "- Градиент функции потерь используется для обновления параметров модели с целью минимизации этой ошибки.\n",
    "\n",
    "Функции потерь\n",
    "\n",
    "- `Кросс-энтропия` (Cross-Entropy Loss): Используется, когда цель — предсказать вероятность для одного из нескольких классов.\n",
    "\n",
    "    - $\\boxed{L = -\\frac{1}{N}\\sum^N \\sum^K y_{i, k} \\cdot log(\\hat{y}_{i, k})}$\n",
    "    - Классификация тональности текста \n",
    "    - Категоризация документов\n",
    "- `Перекрестная энтропия с маскированием`: Используется для задач, где длина последовательностей может различаться (например, Seq2Seq модели)\n",
    "     - $\\boxed{L = -\\frac{1}{N}\\sum^N \\sum^T m_{i,t} \\cdot y_{i, t} \\cdot log(\\hat{y}_{i, t})}$\n",
    "        - $m_{i,t}$: маска (равна 1 для реальных токенов, 0 для паддинга)\n",
    "    - Перевод текста\n",
    "    - Генерация текста (например, автозаполнение).\n",
    "\n",
    "- `Негативное логарифмическое правдоподобие` (Negative Log-Likelihood, NLL): Используется в языковых моделях, чтобы минимизировать вероятность ошибок в предсказании следующего слова.\n",
    "    - $\\boxed{L = -\\frac{1}{N}\\sum^N log(y_i | x_i)}$\n",
    "    - Предсказание следующего слова в предложении.\n",
    "- `Контрастивная потеря` (Contrastive Loss): Применяется для обучения моделей, которые работают с эмбеддингами текста\n",
    "    - $\\boxed{L = (1 - y) \\cdot max(0, d-m) + y\\cdot max(0, m-d)}$\n",
    "        - $d$: расстояние между эмбеддингами\n",
    "        - $m$: пороговое значение\n",
    "        - $y$: истинное значение (0 или 1)\n",
    "    - Поиск похожих текстов\n",
    "\n",
    "- `Среднеквадратичная ошибка` (Mean Squared Error, MSE): Используется, когда требуется предсказать числовое значение\n",
    "    - $\\boxed{L = \\frac{1}{N}\\sum^N (y_i - \\hat{y}_{i})^2}$\n",
    "    - Оценка длины текста.\n",
    "    - Прогнозирование следующего слова в тексте с указанием его частоты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "7. <a id=7>Регуляризация</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Регуляризация` — это совокупность методов, которые предотвращают переобучение (overfitting) модели и помогают ей обобщать данные.   \n",
    "Цель регуляризации — сделать модель более устойчивой к шуму и менее зависимой от конкретных данных в тренировочной выборке, чтобы она хорошо работала на новых данных.  \n",
    "Регуляризация добавляет `ограничения` на `веса` модели или `структуру` нейронной сети, чтобы минимизировать вероятность переобучения.\n",
    "\n",
    "- `L2-регуляризация` (Ridge Regularization)\n",
    "    - $\\boxed{L = L + \\lambda \\cdot \\sum w_i^2}$\n",
    "    - снижает вероятность переобучения, не обнуляя полностью веса\n",
    "    - Чем больше λ, тем сильнее штраф и меньше веса\n",
    "\n",
    "- `L1-регуляризация` (Lasso  Regularization)\n",
    "    - $\\boxed{L = L + \\lambda \\cdot \\sum | w_i|}$\n",
    "    - Полезна для отбора признаков (feature selection), так как обнуляет незначимые веса\n",
    "    - Используется реже в нейронных сетях по сравнению с L2\n",
    "- `Dropout`: На этапе обучения случайно отключает нейроны (и их связи) в сети с заданной вероятностью p, что препятствует избыточной `зависимости` нейронов друг от друга\n",
    "    - Снижает вероятность переобучения за счет создания разнообразных подмножеств сети\n",
    "    - На этапе тестирования все нейроны используются, но их веса уменьшаются пропорционально вероятности p\n",
    "- `Batch Normalization`: Нормализует активации нейронов внутри слоя, стабилизируя обучение, что помогает сети избегать сильных отклонений `значений активаций`\n",
    "\n",
    "- `Data Augmentation`: Искусственное увеличение тренировочных данных за счет их модификаций \n",
    "    - Синонимизация слов.\n",
    "    - Удаление/перестановка слов в предложениях\n",
    "    - Перевод на другой язык\n",
    "\n",
    "- `Ранняя остановка (Early Stopping)`: Прекращает обучение, если ошибка на `валидационной` выборке перестает уменьшаться, даже если на тренировочной выборке она продолжает снижаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "8. <a id=8>Архитектуры нейронных сетей</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Архитектура**       | **Применение**                     | **Плюсы**                                                                                   | **Минусы**                                                                                  |\n",
    "|-----------------------|------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| **FCNN**             | Табличные данные, простые задачи. | - Простота реализации. <br> - Хорошо работает на структурированных данных.                 | - Неподходяща для неструктурированных данных (изображения, текст). <br> - Много параметров. |\n",
    "| **CNN**              | Обработка изображений.            | - Автоматическое извлечение пространственных признаков. <br> - Эффективна для изображений.  | - Ограничена в работе с последовательными данными (например, текст).                       |\n",
    "| **RNN**              | Последовательности.               | - Учитывает временные зависимости. <br> - Подходит для работы с последовательными данными. | - Проблемы исчезающих и взрывающихся градиентов. <br> - Плохо работает с длинными последовательностями. |\n",
    "| **LSTM/GRU**         | Последовательности.               | - Решает проблему длинных зависимостей. <br> - Подходит для текста, временных рядов.        | - Более сложные вычисления. <br> - Медленная обработка по сравнению с трансформерами.      |\n",
    "| **Transformers**     | Текст, последовательности.        | - Эффективны для длинных последовательностей. <br> - Высокая параллелизация.               | - Высокие вычислительные затраты. <br> - Требуют большого объема данных для обучения.      |\n",
    "| **Autoencoders**     | Сжатие данных.                    | - Компактное представление данных. <br> - Применимы для обнаружения аномалий.              | - Не всегда сохраняют детали данных. <br> - Зависимость от правильной настройки.           |\n",
    "| **GANs**             | Генерация данных.                 | - Генерация реалистичных данных. <br> - Применимы для увеличения данных.                   | - Сложность обучения (нестабильная сходимость). <br> - Чувствительность к настройке.       |\n",
    "| **GNN**              | Графовые структуры.               | - Учет сложных связей между объектами. <br> - Подходит для анализа сетей и рекомендаций.    | - Высокая вычислительная сложность на больших графах. <br> - Требует специализированных данных. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "9. <a id=9>Рекуррентные нейронные сети (RNN)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Рекуррентные нейронные сети` (RNN, Recurrent Neural Networks) — это класс нейронных сетей, специально разработанных для обработки последовательных данных, которые обладают механизмом обратной связи, который позволяет им учитывать информацию из предыдущих временных шагов при принятии решений.\n",
    "\n",
    "- На каждом временном шаге $t$ нейронная сеть получает текущий вход $x_t$ и скрытое состояние $h_{t-1}$, которое содержит информацию о предыдущих шагах.\n",
    "    - $\\boxed{h_t = f(W_t\\cdot h_{t-1} + W_x \\cdot x_t + b)}$\n",
    "        - $W_t$: матрица весов для скрытого состояния\n",
    "        - $W_x$: матрица весов для входных данных\n",
    "        - $f$: функция активации ( tanh, ReLU)\n",
    "- На каждом шаге сеть генерирует выход $y_t$:\n",
    "    - $\\boxed{y_t = g(W_y \\cdot h_t + b)}$\n",
    "        - $g$: функция активаци (softmax для классификации)\n",
    "\n",
    "| **Применение**                     | **Плюсы**                                                                                   | **Минусы**                                                                                  |\n",
    "|------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| <br>- Определение тональности текста   <br> - Выделение сущностей в тексте <br> - Анализ временных рядов          | <br> - Учитывают последовательные зависимости в данных | <br> - Проблема исчезающих и взрывающихся градиентов  <br> - Плохая работа с длинными зависимостями <br> - Обработка последовательностей выполняется по шагам, что замедляет обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "10. <a id=10>LSTM и GRU</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LSTM` (Long Short-Term Memory) и `GRU` (Gated Recurrent Unit) — это улучшенные версии рекуррентных нейронных сетей (RNN), которые решают проблемы `исчезающих` и `взрывающихся` градиентов и позволяют моделям запоминать зависимости на длительных временных интервалах. Они используют `гейты`, которые контролируют поток информации внутри сети.\n",
    "\n",
    "`LSTM`(Long Short-Term Memory)\n",
    "\n",
    "<img src='img/lstm.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Гейт забывания` ($f_t$) определяет, какая часть информации из `предыдущего` состояния $C_{t-1}$ будет сохранена:\n",
    "    - $\\boxed{f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b)}$\n",
    "        - $x_t$: входной вектор\n",
    "        - $h_{t-1}$: скрытое состояние на предыдущем шаге\n",
    "        - $[h_{t-1}, x_t]$: Конкатенация векторов\n",
    "\n",
    "- `Гейт ввода` ($i_t$) определяет, какую новую информацию добавить в состояние: \n",
    "    - $\\boxed{i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b)}$\n",
    "\n",
    "- `Кандидат на новое состояние` ($\\~C_t$): \n",
    "    - $\\boxed{\\~C_t = tanh(W_c \\cdot [h_{t-1}, x_t] + b)}$\n",
    "\n",
    "- `Гейт вывода` ($o_t$) определяет, какая часть состояния будет использоваться для генерации скрытого состояния: \n",
    "    - $\\boxed{i_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b)}$\n",
    "\n",
    "`Обновление состояния` ($C_t$) : $\\boxed{C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\~C_t}$\n",
    "\n",
    "`Выходное скрытое состояние` ($h_t$) : $\\boxed{h_t = o_t \\cdot tanh(C_t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GRU` (Gated Recurrent Unit)\n",
    "\n",
    "<img src='img/gru.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Гейт обновления ` ($z_t$) определяет, какая часть информации из `предыдущего` состояния $h_{t-1}$ будет сохранена:\n",
    "    - $\\boxed{z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b)}$\n",
    "\n",
    "- `Гейт сброса  ` ($r_t$) Контролирует, сколько информации из предыдущего состояния используется для вычисления нового состояния:\n",
    "    - $\\boxed{r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b)}$\n",
    "\n",
    "- `Кандидат на новое состояние` ($\\~h_t$): \n",
    "    - $\\boxed{\\~h_t = tanh(W_h \\cdot [r_t \\cdot h_{t-1}, x_t] + b)}$\n",
    "\n",
    "`Обновление состояния` ($h_t$) : $\\boxed{h_t = z_t \\cdot h_{t-1} + (1 - z_t) \\cdot \\~h_t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Архитектура** | **Применение**                                                                 | **Плюсы**                                                                                   | **Минусы**                                                                                  |\n",
    "|------------------|-------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|\n",
    "| **LSTM**        | - Машинный перевод. <br> - Генерация текста. <br> - Прогнозирование временных рядов. | - Учитывает длинные временные зависимости. <br> - Эффективна для сложных последовательностей. | - Высокая вычислительная сложность. <br> - Более длительное время обучения.                |\n",
    "| **GRU**         | - Анализ текста. <br> - Обработка речи. <br> - Краткосрочные временные зависимости. | - Меньше параметров, чем у LSTM. <br> - Быстрее обучается. <br> - Эффективна при ограниченных ресурсах. | - Менее гибкая по сравнению с LSTM. <br> - Может хуже справляться с очень длинными зависимостями. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "11. <a id=11>Механизм внимания (Attention)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Механизм внимания` (Attention) — это метод, используемый в нейронных сетях для выделения `наиболее важных` частей входных данных, которые необходимы для выполнения текущей задачи. Он позволяет модели `фокусироваться` на ключевых элементах входной последовательности, игнорируя менее значимые части.\n",
    "\n",
    "- Дается оценка каждого входного элемента в виде кэффициента\n",
    "- Полученные коэффициенты используются для взвешенного суммирования элементов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Алгоритм self-attention`:\n",
    "\n",
    "$\\boxed{1.}$  Для матрицы `эмбеддингов` токенов последовательности рассчитываются матрицы `Q`, `K`, `V`:\n",
    "- $\\boxed{Q = X \\cdot W^Q}$\n",
    "- $\\boxed{K = X \\cdot W^K}$\n",
    "- $\\boxed{V = X \\cdot W^V}$\n",
    "\n",
    "    - $W^Q, W^K, W^V$ : Обучаемые весовые матрицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [2, 1],\n",
       "        [1, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = torch.tensor(([1,2],   # Эмбеддинг слова Я\n",
    "                  [2,1],   # Эмбеддинг слова люблю\n",
    "                  [1,0]))  # Эмбеддинг слова жизнь\n",
    "\n",
    "W_q = torch.tensor(([1,0],  # Обучаемая весовая матрица Queries\n",
    "                    [0,1]))\n",
    "W_k = torch.tensor(([1,1],  # Обучаемая весовая матрица Keys\n",
    "                    [0,1]))\n",
    "W_v = torch.tensor(([0,1],  # Обучаемая весовая матрица Values\n",
    "                    [1,0]))\n",
    "\n",
    "Q = X @ W_q             # Матрица Queries\n",
    "K = X @ W_k             # Матрица Keys\n",
    "V = X @ W_v             # Матрица Values\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed{2.}$  Рассчитывается матрица \"сырых\" оценок (`логиты`) важности.   \n",
    "В этой матрице строки соответствуют эмбедингам, а в столбцах логиты каждого эмбеддинга исходной последовательности:\n",
    "- $\\boxed{logits = \\frac{Q \\cdot K^T}{\\sqrt{\\text{dim(K)}}}}$\n",
    "     - $dim(K)$: Кол-во столбцов матрицы K, для избежания слишком больших значений логитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.9497, 5.6569, 2.1213],\n",
       "        [3.5355, 4.9497, 2.1213],\n",
       "        [0.7071, 1.4142, 0.7071]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = (Q @ K.T) / np.sqrt(K.shape[1])\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed{3.}$  Каждая строка  логитов преобразуется в `вероятности` при помощи `softmax`:\n",
    "- $\\boxed{ \\text{attention weights = softmax(logits)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3239, 0.6569, 0.0191],\n",
       "        [0.1867, 0.7679, 0.0454],\n",
       "        [0.2483, 0.5035, 0.2483]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_weights = torch.functional.F.softmax(logits, dim=1)\n",
    "att_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed{4.}$  Рассчитываются `контекстные эмбеддинги`, где каждый токен учитывает информацию о других токенах:\n",
    "- $\\boxed{\\text{attention output} =  \\text{attention weights}\\cdot V}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3048, 1.6569],\n",
       "        [1.1413, 1.7679],\n",
       "        [1.0000, 1.5035]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_output = att_weights @ V.float()\n",
    "att_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
